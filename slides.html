<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="author" content="T. Verbeiren" />
  <title>Spark</title>
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">
  <link rel="stylesheet" href="reveal.js-2.6.2/css/reveal.css"/>
    <style type="text/css">code{white-space: pre;}</style>
    <link rel="stylesheet" href="reveal.js-2.6.2/css/theme/simple.css" id="theme">

<!-- Important for javascript graphs -->
    <link rel="stylesheet" href="reveal.js-2.6.2/css/theme/blood.css" id="theme">


    <!-- For syntax highlighting using highlight.js-->
    <link rel="stylesheet" href="reveal.js-2.6.2/lib/css/zenburn.css">

  <link rel="stylesheet" media="print" href="reveal.js-2.6.2/css/print/pdf.css" />
  <!--[if lt IE 9]>
  <script src="reveal.js-2.6.2/lib/js/html5shiv.js"></script>
  <![endif]-->
    <script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
</head>
<body>

<script src="d3.v3.min.js"></script>
<script src="graphlib.min.js"></script>
<script src="dagre-d3.min.js"></script>
  <script src="reveal.js-2.6.2/lib/js/head.min.js"></script>
  <script src="reveal.js-2.6.2/js/reveal.min.js"></script>


  <div class="reveal">
    <div class="slides">

<section>
    <h1 class="title">Spark</h1>
    <h2 class="author">T. Verbeiren</h2>
    <h3 class="date">9/7/2014</h3>
</section>

<section id="contents" class="slide level1">
<h1>Contents</h1>
<p>Introduction</p>
<p>Spark</p>
<p>Ecosystem</p>
<p>Not covered</p>
<p>Example(s)</p>
</section>
<section id="me-myself-and-i" class="slide level1">
<h1>Me, Myself and I</h1>
</section>
<section class="slide level1">

<figure>
<img src="pics/I.png" />
</figure>
</section>
<section id="introduction" class="slide level1">
<h1>Introduction</h1>
</section>
<section class="slide level1">

<h2 id="distribution-is-hard-...">Distribution is hard ...</h2>
<p> </p>
<p>2 world views:</p>
<p> </p>
<ul>
<li><strong>H</strong>igh <strong>P</strong>erformance <strong>C</strong>omputing</li>
<li><strong>H</strong>igh <strong>T</strong>hroughput <strong>C</strong>omputing</li>
</ul>
</section>
<section class="slide level1">

<h2 id="map-reduce">Map / Reduce</h2>
<p> </p>
<h3 id="map"><code>map</code></h3>
<p> </p>
<h3 id="reduce"><code>reduce</code></h3>
<p> </p>
<p>(Nothing special ?!)</p>
</section>
<section class="slide level1">

<h2 id="word-count-in-mr">Word Count in M/R</h2>
</section>
<section class="slide level1">

<figure>
<img src="pics/MapReduceWordCountOverview.png" />
</figure>
</section>
<section class="slide level1">

<pre class="java"><code>import java.io.IOException;
import java.util.*;
        
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.conf.*;
import org.apache.hadoop.io.*;
import org.apache.hadoop.mapreduce.*;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;

public class WordCount {
        
    public static void main(String[] args) throws Exception {
        Configuration conf = new Configuration();
            
        Job job = new Job(conf, &quot;wordcount&quot;);
        
        job.setOutputKeyClass(Text.class);
        job.setOutputValueClass(IntWritable.class);
            
        job.setMapperClass(Map.class);
        job.setReducerClass(Reduce.class);
            
        job.setInputFormatClass(TextInputFormat.class);
        job.setOutputFormatClass(TextOutputFormat.class);
            
        FileInputFormat.addInputPath(job, new Path(args[0]));
        FileOutputFormat.setOutputPath(job, new Path(args[1]));
            
        job.waitForCompletion(true);
     }  

     public static class Map extends Mapper&lt;LongWritable, Text, Text, IntWritable&gt; {
        private final static IntWritable one = new IntWritable(1);
        private Text word = new Text();
            
        public void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {
            String line = value.toString();
            StringTokenizer tokenizer = new StringTokenizer(line);
            while (tokenizer.hasMoreTokens()) {
                word.set(tokenizer.nextToken());
                context.write(word, one);
            }
        }
     } 

     public static class Reduce extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt; {

        public void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context) 
          throws IOException, InterruptedException {
            int sum = 0;
            for (IntWritable val : values) {
                sum += val.get();
            }
            context.write(key, new IntWritable(sum));
        }
     }
}</code></pre>
</section>
<section class="slide level1">

<h2 id="what-if-...">What if ...</h2>
</section>
<section class="slide level1">

<h3 id="we-could-just-write">... we could just write</h3>
<p> </p>
<pre class="scala"><code>val y = x map () reduce ()</code></pre>
</section>
<section class="slide level1">

<h3 id="this-could-be-extended">... this could be extended</h3>
<p> </p>
<pre class="scala"><code>val y = x map () filter() map () flatMap () reduce ()</code></pre>
</section>
<section class="slide level1">

<h2 id="what-would-be-needed">What would be needed?</h2>
<p> </p>
<ol type="1">
<li>Platform</li>
</ol>
<!-- Fast and efficient resource management -->

<p> </p>
<ol start="2" type="1">
<li>Parallel abstraction mechanism</li>
</ol>
<!-- Some kind of abstraction that allows us to deal with what instead of how -->

<p> </p>
<ol start="3" type="1">
<li>Language support</li>
</ol>
<!-- Functions as first-class citizens -->

<p> </p>
</section>
<section id="spark" class="slide level1">
<h1>Spark</h1>
</section>
<section class="slide level1">

<p> </p>
<figure>
<img src="pics/spark-logo.png" />
</figure>
<p> </p>
<ul>
<li>Berkeley University</li>
<li>Apache Project</li>
<li>v1.0 released on May 30d 2014</li>
<li>Written in Scala</li>
<li>Supported by <a href="http://databricks.com/">DataBricks</a></li>
<li>Used by ...</li>
</ul>
</section>
<section class="slide level1">

<h2 id="language-support">Language support</h2>
<p><br /> <img src="pics/Languages.png" /></p>
</section>
<section class="slide level1">

<h2 id="platform">Platform</h2>
<p> </p>
<p>Built for low-latency</p>
</section>
<section class="slide level1">

<h2 id="abststraction-mechanism">Abststraction mechanism</h2>
<p> </p>
<p><strong>R</strong>esilient <strong>D</strong>istributed <strong>D</strong>atasets</p>
</section>
<section class="slide level1">

<h3 id="rdds">RDDs</h3>
<p> </p>
<p><em>Immutable</em> Collection</p>
<p> </p>
<p>Accepting <strong>transformations</strong> and <strong>actions</strong></p>
</section>
<section class="slide level1">

<h3 id="transformations">Transformations</h3>
<p> </p>
<ul>
<li><code>map</code></li>
<li><code>filter</code></li>
<li><code>sample</code></li>
<li><code>union</code> / <code>intersection</code></li>
<li><code>groupByKey</code></li>
<li><code>reduceByKey</code></li>
<li><code>join</code></li>
<li>...</li>
</ul>
</section>
<section class="slide level1">

<h3 id="actions">Actions</h3>
<p> </p>
<ul>
<li><code>reduce</code></li>
<li><code>collect</code></li>
<li><code>count</code></li>
<li><code>take(n)</code></li>
<li><code>saveAsTextFile</code></li>
<li>...</li>
</ul>
</section>
<section class="slide level1">

<h3 id="the-first-example">The first example</h3>
<p> </p>
<svg width="400" height="400">
  
<rect width="400" height="400" style="fill:rgb(0,0,0);stroke-width:0;stroke:rgb(0,0,0)" /> <circle cx="200" cy="200" r="200" stroke="black" stroke-width="0" fill="yellow" /> <line x1="200" y1="200" x2="360" y2="80" style="stroke:rgb(0,0,0);stroke-width:4" /> <text x="240" y="80" fill="black">r = 1/2</text>
</svg>

<p> </p>
<p><span class="math">\[\text{P}(\text{hitting circle}) \approx
\text{Surface circle} = \frac{\pi}{4}\]</span></p>
</section>
<section class="slide level1">

<pre class="scala"><code>import sc._

val N = 10000000

// Generate a sequence of numbers and distribute
val par = parallelize(1 to N)

// Generate a point in 2D unit square
def randomPoint:(Double,Double) = {
    val x = Math.random()
    val y = Math.random()
    (x,y)
}
// Check if a point lies in the unit circle
def inCircle(point:(Double,Double)):Int = {
    if (point._1*point._1 + point._2*point._2 &lt; 1) 1 else 0
}</code></pre>
</section>
<section class="slide level1">

<pre class="scala"><code>// List of hits yes/no
val inCircleList = par map(i =&gt; inCircle(randomPoint))

// Return the first 5 elements from the RDD
inCircleList take 5

// Get info about the RDD
inCircleList.toDebugString

// The number of hits
val total = inCircleList reduce (_+_)

// Probability of hitting the circle *4 = Pi
val S = 4. * total / N</code></pre>
</section>
<section class="slide level1">

<p>From the <a href="https://spark.apache.org/examples.html">Spark examples</a> page</p>
<p><br /></p>
<pre class="scala"><code>val count = parallelize(1 to N).map{i =&gt;
  val x = Math.random()
  val y = Math.random()
  if (x*x + y*y &lt; 1) 1 else 0
}.reduce(_ + _)
println(&quot;Pi is roughly &quot; + 4.0 * count / N)</code></pre>
</section>
<section class="slide level1">

<p>Hadoop M/R in Spark</p>
<p> </p>
<pre class="scala"><code>// Read a file from Hadoop FS, (e.g. Ulysses / Project Gutenberg)
// and process it similar to Hadoop M/R
val file = textFile(&quot;Joyce-Ulysses.txt&quot;)

// Convert to an array of words in the text
val words = file.flatMap(_.split(&quot; &quot;))

// Map to (key,value) pairs
val mapped = words map (word =&gt; (word,1))

// Sort and group by key, 
// Result is of form (key, List(value1, value2, value3, ...))
val grouped = mapped sortByKey() groupByKey()

// The length of the values array yields the amount
val result = grouped map {case (k,vs) =&gt; (k,vs.length)}
// But where is the *reduce*?</code></pre>
</section>
<section class="slide level1">

<p>Be careful with <em>definitions</em> of <code>map</code> and <code>reduce</code>!</p>
<p> </p>
<pre class="scala"><code>// Read a file from Hadoop FS, (e.g. Ulysses / Project Gutenberg)
// and process it similar to Hadoop M/R
val file = textFile(&quot;Joyce-Ulysses.txt&quot;)

// Convert to an array of words in the text
val words = file.flatMap(_.split(&quot; &quot;))

// Map to (key,value) pairs
val mapped = words map (word =&gt; (word,1)) 

// Sort and group by key, 
// Result is of form (key, List(value1, value2, value3, ...))
val grouped = mapped sortByKey() groupByKey()

// The length of the values array yields the amount
// val result = grouped map {case (k,vs) =&gt; (k,vs.length)}
val result = grouped map {case (k,vs) =&gt; (k, vs reduce (_+_))}</code></pre>
</section>
<section class="slide level1">

<p>In Spark, we would use:</p>
<p> </p>
<pre class="scala"><code>val file = textFile(&quot;Joyce-Ulysses.txt&quot;)
val words = file.flatMap(_.split(&quot; &quot;))
val mapped = words map (word =&gt; (word,1))
val result = mapped reduceByKey(_+_)
result collect</code></pre>
</section>
<section class="slide level1">

<h2 id="and-repl">... and REPL</h2>
</section>
<section class="slide level1">

<h2 id="and-web-interface">... and Web interface</h2>
</section>
<section class="slide level1">

<figure>
<img src="pics/SparkInterface.png" />
</figure>
</section>
<section class="slide level1">

<h2 id="and-distributed-memory-caching">... and distributed memory caching</h2>
<p> </p>
<pre class="scala"><code>val file = textFile(&quot;Joyce-Ulysses.txt&quot;)
val words = file.flatMap(_.split(&quot; &quot;))
val mapped = words map (word =&gt; (word,1))
// Cache the RDD for later use
val cached = mapped cache()
// Use the cached version
val result = cached reduceByKey(_+_)
// Oops, nothing happens?
result.collect
// Laziness... oh my
result.collect

// Count how many times the word &#39;the&#39; occurs in the text
cached filter {case(word,v) =&gt; word==&quot;the&quot;} reduceByKey(_+_) collect</code></pre>
<!-- So yes, caching works... but also: highly interactive! -->

</section>
<section class="slide level1">

<h2 id="and-interactive-use">... and interactive use</h2>
</section>
<section class="slide level1">

<h2 id="and-the-ecosystem">... and the Ecosystem</h2>
<p> </p>
<ul>
<li>Spark SQL: <a href="http://spark.apache.org/sql/">http://spark.apache.org/sql/</a></li>
<li>Spark Streaming: <a href="http://spark.apache.org/streaming/">http://spark.apache.org/streaming/</a></li>
<li>BlinkDB: <a href="http://blinkdb.org/">http://blinkdb.org/</a></li>
<li>MLlib: <a href="http://spark.apache.org/mllib/">http://spark.apache.org/mllib/</a></li>
<li>GraphX: <a href="http://spark.apache.org/graphx/">http://spark.apache.org/graphx/</a></li>
<li>SparkR: <a href="http://amplab-extras.github.io/SparkR-pkg/">http://amplab-extras.github.io/SparkR-pkg/</a></li>
</ul>
</section>
<section id="not-covered" class="slide level1">
<h1>Not covered</h1>
</section>
<section class="slide level1">

<h2 id="better">Better ?</h2>
<h2 id="faster">Faster ?</h2>
<h2 id="easier">Easier ?</h2>
<h2 id="section">...</h2>
</section>
<section class="slide level1">

<h2 id="rdds-under-the-hood">RDDs Under the Hood</h2>
<p> </p>
<p><a href="http://dl.acm.org/citation.cfm?id=2228301">http://dl.acm.org/citation.cfm?id=2228301</a></p>
<p> </p>
<figure>
<img src="pics/RDDpaper-full.png" />
</figure>
</section>
<section class="slide level1">

<h2 id="configuration-performance">Configuration &amp; Performance</h2>
<p> </p>
<p><a href="https://spark.apache.org/docs/latest/configuration.html">https://spark.apache.org/docs/latest/configuration.html</a></p>
<p><a href="https://spark.apache.org/docs/latest/tuning.html">https://spark.apache.org/docs/latest/tuning.html</a></p>
<p> </p>
<figure>
<img src="pics/PerformanceTuning.png" />
</figure>
</section>
<section class="slide level1">

<h2 id="installation-deployment">Installation &amp; Deployment</h2>
<p> </p>
<p><a href="https://spark.apache.org/docs/latest/cluster-overview.html">https://spark.apache.org/docs/latest/cluster-overview.html</a></p>
<p> </p>
<figure>
<img src="pics/Deployment.png" />
</figure>
</section>
<section id="examples" class="slide level1">
<h1>Example(s)</h1>
</section>
<section class="slide level1">

<h2 id="genomic-data">Genomic Data</h2>
<!-- Terrabytes of data roll out of sequencing machines daily. What are we going to do with all this data? -->

</section>
<section class="slide level1">

<figure>
<img src="pics/genome-glossary.jpg" alt="http://www.broadinstitute.org/education/glossary/genome" /><figcaption><a href="http://www.broadinstitute.org/education/glossary/genome">http://www.broadinstitute.org/education/glossary/genome</a></figcaption>
</figure>
</section>
<section class="slide level1">

<!-- Human DNA consists of around 3 billion base-pairs. this amounts to approximately 3GB of hard drive storage. This is not too much. The complexity of DNA analysis is due to something else: 

1. The machines that read the DNA usually do not do that in an orderly fashion
2. They make a lot of errors when reading base-pairs
3. In order to cope with the errors, multiple copies of the DNA are read

A whole pipeline is then executed in order to reconstruct the original DNA.

Below, we deal with 2 types of information about DNA: coverage information and transcription factors.
 -->

<p>3 billion base pairs (<span class="math">\(3.2 \times 10^9\)</span>)</p>
<p>Packaged in chromosomes</p>
<p>~ 3GB for one human</p>
<p> </p>
<p>Analysis requires a lot of processing power and storage</p>
<p> </p>
<p><strong>Transcription Factors</strong>: proteins that bind on region</p>
<p><strong>Coverage</strong>: basepair occurence in sequencing</p>
</section>
<section class="slide level1">

<p>Coverage data:</p>
<p> </p>
<pre><code>Chromosome   Position    Sequencing coverage
19           11004       1
19           11005       2
19           11006       2
19           11007       2
19           11008       3
19           11009       3</code></pre>
</section>
<section class="slide level1">

<p>Transcription Factor data:</p>
<p> </p>
<pre class="bash"><code>&gt; awk &#39;BEGIN {srand()} !/^$/ { if (rand() &lt;= .00001) print $0}&#39; bedfile.bed

chr1    70529738    70529754    Maf     .   -
chr1    161676477   161676495   Pou2f2  .   -
chr1    176484690   176484699   AP-1    .   -
chr10   6020071     6020084     CTCF    .   -
chr11   1410823     1410838     NF-Y    .   -
chr16   4366053     4366067     YY1     .   +
chr17   77824593    77824602    BAF155  .   +
chr19   10947006    10947013    Rad21   .   -
chr19   49342112    49342121    SIX5    .   +
chr22   39548908    39548922    Irf     .   +
chr7    100048475   100048485   Egr-1   .   -
chr8    119123364   119123374   YY1     .   +
chr8    128562635   128562649   p300    .   -
chr9    14315969    14315982    Egr-1   .   -
chrX    101409366   101409384   CTCF    .   +</code></pre>
</section>
<section class="slide level1">

<pre class="scala"><code>// Load files from HDFS
val covFile = sc.textFile(&quot;NA12878.chrom19.SLX.maq.SRP000032.2009_07.coverage&quot;,8)
val bedFile = sc.textFile(&quot;201101_encode_motifs_in_tf_peaks.bed&quot;,8)

// Class to hold records from coverage data
class covData(val chr: String, val pos: Int, val cov: Int) {
    def this(line: Array[String]) {
     this(line(0).toString, line(1).toInt, line(2).toInt)
    }
}

// Class to hold records from Transcription Factor data
class tfsData(val chr: String, val pos1: Int, val pos2:Int, val tf: String) {
    def this(line: Array[String]) {
     this(line(0).toString, line(1).toInt, line(2).toInt, line(3).toString)
    }
}</code></pre>
</section>
<section class="slide level1">

<pre class="scala"><code>// Turn input files into an RDD of objects
val cov = covFile.map(_.split(&quot;\\s+&quot;)).map(new covData(_))
val tfs = bedFile.map(_.split(&quot;\\s+&quot;)).map(new tfsData(_))

// Count the number of items in both datasets
cov.count
tfs.count

// Cache in memory
val ccov = cov cache
val ctfs = tfs cache

// Count once for the caching to occur
ccov.count
ctfs.count</code></pre>
</section>
<section class="slide level1">

<pre class="scala"><code>// Turn coverage data into K/V pairs
val kvcov = ccov.map(x =&gt; (x.pos,(x.cov))).cache
// Turn TF data into K/V pairs
val kvtfs = ctfs.filter(x =&gt; x.chr == &quot;chr19&quot;).map(x =&gt; (x.pos1,(x.pos2,x.tf)))

// Activate the caching of the coverage data
kvcov.count

// Join both datasets together by key
val cjoined = kvcov.join(kvtfs)

// Waaaw, that&#39;s fast! In fact, nothing happened yet.
// select 5 entries to see the result but reformat first
val flatjoined = cjoined map { case(x,(y,(z,zz))) =&gt; (x,z,zz,y) }
flatjoined take 5

flatjoined.toDebugString</code></pre>
</section>
<section class="slide level1">

<h2 id="visualization-of-big-data">Visualization of Big Data</h2>
<!-- This is really the reason we invest in Spark -->

</section>
<section class="slide level1">

<h3 id="visual-analytics">Visual Analytics</h3>
<figure>
<img src="pics/BD.png" />
</figure>
</section>
<section class="slide level1">

<figure>
<img src="pics/BinnedAggregation.png" />
</figure>
</section>
<section class="slide level1">

<figure>
<img src="pics/layers.png" />
</figure>
</section>
<section class="slide level1">

<figure>
<img src="pics/interface2.png" />
</figure>
</section>
<section class="slide level1">

<figure>
<img src="pics/locustree.png" />
</figure>
</section>
<section class="slide level1">

<figure>
<img src="pics/lazytree.png" alt="Lazy functional tree zipper" /><figcaption>Lazy functional tree zipper</figcaption>
</figure>
</section>
<section class="slide level1">

<p>Part of <code>treeDraw</code> :</p>
<pre class="javascript"><code>  d3.select(window).on(&quot;keydown&quot;, function() {
    d3.event.preventDefault();
    switch (d3.event.keyCode) {
      case 38: 
        zoomOut(tz,treeDraw);
        break ;
      case 40:
        zoomIn(0,tz,treeDraw);
        break;
      case 37: 
        panLeft(tz,treeDraw);
        break;
      case 39: 
        panRight(tz,treeDraw);
        break; 
    };
  });</code></pre>
</section>
<section class="slide level1">

<pre class="scala"><code>object CovQuery extends SparkJob with NamedRddSupport {

  type Range = (Int, Int)
  type TreePath = List[Int]

  val B = 100     // number of bins, also N
  val Z = 10      // zoom level with every step

  override def validate(sc: SparkContext, config: Config): SparkJobValidation = SparkJobValid

  override def runJob(sc: SparkContext, config: Config): Any = {

    // a map with the persistent RDDs
    val listOfPersistentRDDs = sc.getPersistentRDDs

    val chrCache = namedRdds.get[DataPoint](myRDD).get

    // Info about the interval
    val superMin = low
    val superMax = high

    val subRange: Range = calcRange(path,superMin,superMax)
    val mn = subRange._1
    val mx = subRange._2

    // The bin width, based on the number of bins (N)
    val delta = (mx - mn) / B

    // Only filter the region within the interval
    val chrCache1 = chrCache filter (x =&gt; (mn &lt;= x.position) &amp;&amp; (x.position &lt;= mx))

    // Create the bins as the first argument of a new triple
    val x1 = chrCache1 map (x =&gt; (myDiv(x.position - mn, delta), x.position, x.coverage))

    // sum, max and min can be done on only the bin number and the coverage:
    val fork1 = x1.map(x =&gt; (x._1,x._3))

    // The 4 statistics
    val sumSet = fork1.reduceByKey(_ + _)
    val maxSet = fork1.reduceByKey(Math.max(_,_))
    val minSet = fork1.reduceByKey(Math.min(_,_))
    val countSet = x1.map(x =&gt; (x._1,1)).reduceByKey(_+_)

    // Join the data and flatten the (nested) result
    val joined = sumSet.join(maxSet).join(minSet).join(countSet)
    val collected = joined.collect().map(x =&gt; (x._1, flattenT3(x._2)))

    // The boundaries in the original coordinate can not be derived
    // from the data, because it can not cope with empty regions.
    val lowerboundRange = mn until mx by delta
    val lowerboundIndex = 0 until B
    val lowerbound = lowerboundIndex zip lowerboundRange

    val dataMap =collected.map(x=&gt; (x._1,List(x._2._1,x._2._2,x._2._3,x._2._4,x._2._1/x._2._4))).toMap.withDefaultValue(List(0,0,0,0,0))

    return lowerbound.map(x =&gt; (x._1,List(x._2) ++ dataMap(x._1)))

  }</code></pre>
</section>
<section class="slide level1">

<figure>
<img src="pics/Results.png" />
</figure>
</section>
<section class="slide level1">

<p>Paper submitted to LDAV 2014</p>
<p> </p>
<figure>
<img src="pics/LDAV.png" />
</figure>
</section>
<section id="the-end" class="slide level1">
<h1>The end</h1>
<p> </p>
<p>Some links:</p>
<ul>
<li><span class="citation" data-cites="tverbeiren">@tverbeiren</span></li>
<li>Slides: <a href="https://github.com/tverbeiren/BigDataBe-Spark">https://github.com/tverbeiren/BigDataBe-Spark</a></li>
<li>Spark Home: <a href="https://spark.apache.org/">https://spark.apache.org/</a></li>
<li>Data Visualization Lab: <a href="http://datavislab.org">http://datavislab.org</a></li>
<li>ExaScience Life Lab: <a href="http://www.exascience.com/">http://www.exascience.com/</a></li>
<li>Data Intuitive: <a href="http://data-intuitive.com">http://data-intuitive.com</a></li>
</ul>
</section>
    </div>
  </div>




  <script>

      // Full list of configuration options available here:
      // https://github.com/hakimel/reveal.js#configuration
      Reveal.initialize({
        controls: false,
        progress: true,
        history: true,
        center: true,
        // Factor of the display size that should remain empty around the content
        margin: 0.1,
        theme: 'moon', // available themes are in /css/theme
        transition: 'fade', // default/cube/page/concave/zoom/linear/fade/none

        // Optional libraries used to extend on reveal.js
        dependencies: [
          { src: 'reveal.js-2.6.2/lib/js/classList.js', condition: function() { return !document.body.classList; } },
//          { src: 'reveal.js-2.6.2/plugin/zoom-js/zoom.js', async: true, condition: function() { return !!document.body.classList; } },
//          { src: 'reveal.js-2.6.2/plugin/notes/notes.js', async: true, condition: function() { return !!document.body.classList; } },
          { src: 'reveal.js-2.6.2/plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } },
//          { src: 'reveal.js-2.6.2/plugin/search/search.js', async: true, condition: function() { return !!document.body.classList; }, }
//          { src: 'reveal.js-2.6.2/plugin/remotes/remotes.js', async: true, condition: function() { return !!document.body.classList; } }
]});
    </script>
  </body>
</html>
